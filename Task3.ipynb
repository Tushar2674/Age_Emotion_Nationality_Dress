{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0e130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout ,Input \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d44b0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1697 images belonging to 51 classes.\n",
      "Found 125 images belonging to 51 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory paths\n",
    "age_train_dir = 'dataset/Age/train'\n",
    "age_validation_dir = 'dataset/Age/test'\n",
    "\n",
    "# Image data generators\n",
    "age_train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "age_train_generator = age_train_datagen.flow_from_directory(\n",
    "    age_train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "age_validation_generator = age_train_datagen.flow_from_directory(\n",
    "    age_validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d656431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_2740\\2528729424.py:14: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  age_base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(150,150, 3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir_task3\\Age_model\\tuner0.json\n",
      "Epoch 1/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 713ms/step - accuracy: 0.9919 - loss: 0.0602 - val_accuracy: 0.1680 - val_loss: 4.0469\n",
      "Epoch 2/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 675ms/step - accuracy: 0.9899 - loss: 0.0382 - val_accuracy: 0.1600 - val_loss: 4.1501\n",
      "Epoch 3/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 672ms/step - accuracy: 0.9929 - loss: 0.0311 - val_accuracy: 0.1600 - val_loss: 4.1572\n",
      "Epoch 4/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 673ms/step - accuracy: 0.9915 - loss: 0.0355 - val_accuracy: 0.1520 - val_loss: 3.9818\n",
      "Epoch 5/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 673ms/step - accuracy: 0.9906 - loss: 0.0391 - val_accuracy: 0.1600 - val_loss: 4.0574\n",
      "Epoch 6/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 675ms/step - accuracy: 0.9935 - loss: 0.0251 - val_accuracy: 0.1680 - val_loss: 4.2192\n",
      "Epoch 7/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 671ms/step - accuracy: 0.9927 - loss: 0.0217 - val_accuracy: 0.1600 - val_loss: 4.2299\n",
      "Epoch 8/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 670ms/step - accuracy: 0.9910 - loss: 0.0306 - val_accuracy: 0.1760 - val_loss: 4.2451\n",
      "Epoch 9/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 672ms/step - accuracy: 0.9923 - loss: 0.0214 - val_accuracy: 0.1440 - val_loss: 4.3322\n",
      "Epoch 10/10\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 671ms/step - accuracy: 0.9926 - loss: 0.0273 - val_accuracy: 0.1360 - val_loss: 4.3741\n",
      "Found 730 images belonging to 51 classes.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 507ms/step - accuracy: 0.1072 - loss: 4.4078\n",
      "Best Age Model Test Accuracy: 0.10\n",
      "Best Age Model Test Loss: 4.52\n"
     ]
    }
   ],
   "source": [
    "# This is to enhance the accuracy of the age model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import os\n",
    "# Verify number of classes\n",
    "age_num_classes = age_train_generator.num_classes\n",
    "print(f'Number of classes: {age_num_classes}')\n",
    "\n",
    "# Pre-trained model\n",
    "age_base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(150,150, 3))\n",
    "\n",
    "# Fine-tune the model\n",
    "age_base_model.trainable = True\n",
    "fine_tune_at = 100\n",
    "for layer in age_base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Function to build the model (for hyperparameter tuning)\n",
    "def age_build_model(hp):\n",
    "    age_model = Sequential([\n",
    "        age_base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(hp.Int('units', min_value=128, max_value=512, step=64), activation='relu'),\n",
    "        Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)),\n",
    "        Dense(age_num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    age_model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return age_model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "tuner = RandomSearch(\n",
    "    age_build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=12,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir_task3',\n",
    "    project_name='Age_model'\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "tuner.search(age_train_generator, validation_data=age_validation_generator, epochs=10)\n",
    "\n",
    "# Get the best model\n",
    "best_age_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Further train the best model\n",
    "best_age_model.fit(age_train_generator, validation_data=age_validation_generator, epochs=10)\n",
    "\n",
    "# Save the best model\n",
    "best_age_model.save('best_age_model.keras')\n",
    "\n",
    "# Evaluate the best model\n",
    "age_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "age_test_generator = age_test_datagen.flow_from_directory(age_validation_dir, target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
    "\n",
    "test_loss, test_accuracy = best_age_model.evaluate(age_test_generator)\n",
    "print(f'Best Age Model Test Accuracy: {test_accuracy:.2f}')\n",
    "print(f'Best Age Model Test Loss: {test_loss:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6e83b",
   "metadata": {},
   "source": [
    "# emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d1db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5068 images belonging to 6 classes.\n",
      "Found 675 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory paths\n",
    "emotion_train_dir = 'dataset/Emotion/train'\n",
    "emotion_validation_dir = 'dataset/Emotion/test'\n",
    "\n",
    "# Image data generators\n",
    "emotion_train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "emotion_train_generator = emotion_train_datagen.flow_from_directory(\n",
    "    emotion_train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "emotion_validation_generator = emotion_train_datagen.flow_from_directory(\n",
    "    emotion_validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f42a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential([\n",
    "        Input(shape=(150, 150, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(6, activation='softmax') \n",
    "    ])\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f8107c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 922ms/step - accuracy: 0.1916 - loss: 1.9243 - val_accuracy: 0.1793 - val_loss: 1.7012\n",
      "Epoch 2/10\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 942ms/step - accuracy: 0.2476 - loss: 1.7122 - val_accuracy: 0.3452 - val_loss: 1.6254\n",
      "Epoch 3/10\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 917ms/step - accuracy: 0.3296 - loss: 1.6204 - val_accuracy: 0.3304 - val_loss: 1.6298\n",
      "Epoch 4/10\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 946ms/step - accuracy: 0.4088 - loss: 1.4974 - val_accuracy: 0.3467 - val_loss: 1.6507\n",
      "Epoch 5/10\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 824ms/step - accuracy: 0.5124 - loss: 1.2700 - val_accuracy: 0.3526 - val_loss: 1.6505\n",
      "Epoch 6/10\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 799ms/step - accuracy: 0.6283 - loss: 1.0152 - val_accuracy: 0.3719 - val_loss: 1.7814\n",
      "Epoch 7/10\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 778ms/step - accuracy: 0.7361 - loss: 0.7312 - val_accuracy: 0.3659 - val_loss: 2.1496\n",
      "Epoch 8/10\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 711ms/step - accuracy: 0.8225 - loss: 0.5167 - val_accuracy: 0.3630 - val_loss: 2.4832\n",
      "Epoch 9/10\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 896ms/step - accuracy: 0.8889 - loss: 0.3440 - val_accuracy: 0.3585 - val_loss: 3.0946\n",
      "Epoch 10/10\n",
      "\u001b[1m159/159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 912ms/step - accuracy: 0.9234 - loss: 0.2592 - val_accuracy: 0.3393 - val_loss: 3.4814\n"
     ]
    }
   ],
   "source": [
    "history_emotion = emotion_model.fit(\n",
    "    emotion_train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=emotion_validation_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1c32843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 479ms/step - accuracy: 0.1346 - loss: 4.3223\n",
      "Age recognition accuracy: 13.60%\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.1223 - loss: 1.9505\n",
      "Emotion detection accuracy: 12.69%\n"
     ]
    }
   ],
   "source": [
    "# Age recognition evaluation\n",
    "age_loss, age_accuracy = best_age_model.evaluate(age_validation_generator)\n",
    "print(f'Age recognition accuracy: {age_accuracy * 100:.2f}%')\n",
    "\n",
    "# Emotion detection evaluation\n",
    "emotion_loss, emotion_accuracy = emotion_model.evaluate(emotion_validation_generator)\n",
    "print(f'Emotion detection accuracy: {emotion_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaaae4d",
   "metadata": {},
   "source": [
    "# Nationality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea95280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1385 images belonging to 4 classes.\n",
      "Found 49 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory paths\n",
    "nation_train_dir = 'dataset/Nationality/train'\n",
    "nation_validation_dir = 'dataset/Nationality/test'\n",
    "\n",
    "# Image data generators\n",
    "nation_train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "nation_train_generator = nation_train_datagen.flow_from_directory(\n",
    "    nation_train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "nation_validation_generator = nation_train_datagen.flow_from_directory(\n",
    "    nation_validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5da7c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_model = Sequential([\n",
    "        Input(shape=(150, 150, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')  \n",
    "    ])\n",
    "\n",
    "nation_model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f82fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 818ms/step - accuracy: 0.6493 - loss: 1.0402 - val_accuracy: 0.5306 - val_loss: 1.1387\n",
      "Epoch 2/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 682ms/step - accuracy: 0.7001 - loss: 0.8255 - val_accuracy: 0.5306 - val_loss: 1.1095\n",
      "Epoch 3/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 675ms/step - accuracy: 0.7136 - loss: 0.7384 - val_accuracy: 0.5102 - val_loss: 1.2349\n",
      "Epoch 4/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 670ms/step - accuracy: 0.7667 - loss: 0.6444 - val_accuracy: 0.5510 - val_loss: 1.6767\n",
      "Epoch 5/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 677ms/step - accuracy: 0.7680 - loss: 0.5676 - val_accuracy: 0.5714 - val_loss: 1.5454\n",
      "Epoch 6/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 678ms/step - accuracy: 0.8167 - loss: 0.4642 - val_accuracy: 0.5306 - val_loss: 1.8626\n",
      "Epoch 7/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 688ms/step - accuracy: 0.8510 - loss: 0.3791 - val_accuracy: 0.5102 - val_loss: 2.0892\n",
      "Epoch 8/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 685ms/step - accuracy: 0.8717 - loss: 0.3245 - val_accuracy: 0.4898 - val_loss: 2.7135\n",
      "Epoch 9/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 688ms/step - accuracy: 0.9097 - loss: 0.2167 - val_accuracy: 0.4898 - val_loss: 3.7122\n",
      "Epoch 10/10\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 689ms/step - accuracy: 0.9503 - loss: 0.1498 - val_accuracy: 0.5306 - val_loss: 3.3630\n"
     ]
    }
   ],
   "source": [
    "history_nation = nation_model.fit(\n",
    "    nation_train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=nation_validation_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a34ca",
   "metadata": {},
   "source": [
    "# Dress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37a9097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 10 classes.\n",
      "Found 2 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directory paths\n",
    "dress_train_dir = 'dataset/Dress/train'\n",
    "dress_validation_dir = 'dataset/Dress/test'\n",
    "\n",
    "# Image data generators\n",
    "dress_train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "dress_train_generator = dress_train_datagen.flow_from_directory(\n",
    "    dress_train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "dress_validation_generator = dress_train_datagen.flow_from_directory(\n",
    "    dress_validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3a7484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dress_model = Sequential([\n",
    "        Input(shape=(150, 150, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(10, activation='softmax')  \n",
    "    ])\n",
    "\n",
    "dress_model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc33aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 724ms/step - accuracy: 0.2111 - loss: 2.1516 - val_accuracy: 0.5000 - val_loss: 1.4266\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 638ms/step - accuracy: 0.6131 - loss: 1.0899 - val_accuracy: 0.5000 - val_loss: 1.2900\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 662ms/step - accuracy: 0.7686 - loss: 0.6669 - val_accuracy: 1.0000 - val_loss: 0.0496\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 602ms/step - accuracy: 0.8125 - loss: 0.4514 - val_accuracy: 1.0000 - val_loss: 0.0032\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 642ms/step - accuracy: 0.8811 - loss: 0.2817 - val_accuracy: 1.0000 - val_loss: 2.3539e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 635ms/step - accuracy: 0.9119 - loss: 0.2776 - val_accuracy: 1.0000 - val_loss: 2.6179e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 605ms/step - accuracy: 0.8711 - loss: 0.4257 - val_accuracy: 1.0000 - val_loss: 6.2808e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 644ms/step - accuracy: 0.8004 - loss: 0.6478 - val_accuracy: 1.0000 - val_loss: 9.0604e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 606ms/step - accuracy: 0.8639 - loss: 0.3891 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 605ms/step - accuracy: 0.9433 - loss: 0.2311 - val_accuracy: 1.0000 - val_loss: 8.7861e-04\n"
     ]
    }
   ],
   "source": [
    "history_dress = dress_model.fit(\n",
    "    dress_train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=dress_validation_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0a085e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.5412 - loss: 3.2854\n",
      "Nation recognition accuracy: 53.06%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 8.7861e-04\n",
      "Dress detection accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# nationality evaluation\n",
    "nation_loss, nation_accuracy = nation_model.evaluate(nation_validation_generator)\n",
    "print(f'Nation recognition accuracy: {nation_accuracy * 100:.2f}%')\n",
    "\n",
    "# dress evaluation\n",
    "dress_loss, dress_accuracy = dress_model.evaluate(dress_validation_generator)\n",
    "print(f'Dress detection accuracy: {dress_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca60fad",
   "metadata": {},
   "source": [
    "# saving model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9061d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the age model\n",
    "best_age_model.save('age_model.keras')\n",
    "\n",
    "# Save the emotion detection model\n",
    "emotion_model.save('emotion_detection_model.keras')\n",
    "\n",
    "# Save the nation model\n",
    "nation_model.save('nation_model.keras')\n",
    "\n",
    "# Save the dress color detection model\n",
    "dress_model.save('dress_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2019301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for making classes \n",
    "import json\n",
    "\n",
    "# Assuming train_generator is your training data generator\n",
    "nationality_class_indices = nation_train_generator.class_indices\n",
    "emotion_class_indices = emotion_train_generator.class_indices\n",
    "dress_color_class_indices = dress_train_generator.class_indices\n",
    "\n",
    "# Save to JSON files\n",
    "with open('nationality_class_indices.json', 'w') as f:\n",
    "    json.dump(nationality_class_indices, f)\n",
    "\n",
    "with open('emotion_class_indices.json', 'w') as f:\n",
    "    json.dump(emotion_class_indices, f)\n",
    "\n",
    "with open('dress_color_class_indices.json', 'w') as f:\n",
    "    json.dump(dress_color_class_indices, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854166a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
